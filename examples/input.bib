@inproceedings{scann,
  title  = {Accelerating Large-Scale Inference with Anisotropic Vector Quantization},
  author = {Ruiqi Guo and Philip Sun and Erik Lindgren and Quan Geng and David Simcha and Felix Chern and Sanjiv Kumar},
  year   = {2020}
}

@proceedings{wassa2021approaches,
    title = {Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},
    editor = {De Clercq, Orphee  and Balahur, Alexandra  and Sedoc, Joao  and Barriere, Valentin  and Tafreshi, Shabnam  and Buechel, Sven  and Hoste, Veronique}
}

@software{spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}

@article{davis2015commonsense,
  title     = {Commonsense reasoning and commonsense knowledge in artificial intelligence},
  author    = {Davis, Ernest and Marcus, Gary},
  journal   = {Communications of the ACM},
  volume    = {58},
  number    = {9},
  pages     = {92--103},
  year      = {2015},
  publisher = {ACM New York, NY, USA}
}

@inproceedings{Singh2002OpenMC,
  title        = {Open Mind Common Sense: Knowledge acquisition from the general public},
  author       = {Singh, Push and Lin, Thomas and Mueller, Erik T and Lim, Grace and Perkins, Travell and Zhu, Wan Li},
  booktitle    = {OTM Confederated International Conferences" On the Move to Meaningful Internet Systems"},
  pages        = {1223--1237},
  year         = {2002},
  organization = {Springer}
}
@inproceedings{Lin2014MicrosoftCC,
  title        = {Microsoft coco: Common objects in context},
  author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle    = {European conference on computer vision},
  pages        = {740--755},
  year         = {2014},
  organization = {Springer}
}

@inproceedings{yao2019plan,
  title     = {Plan-and-write: Towards better automatic storytelling},
  author    = {Yao, Lili and Peng, Nanyun and Weischedel, Ralph and Knight, Kevin and Zhao, Dongyan and Yan, Rui},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {33},
  pages     = {7378--7385},
  year      = {2019}
}


@article{bao2020unilmv2,
  title   = {UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training},
  author  = {Hangbo Bao and Li Dong and Furu Wei and Wenhui Wang and Nan Yang and Xiulei Liu and Yu Wang and Songhao Piao and Jianfeng Gao and Ming Zhou and Hsiao-Wuen Hon},
  journal = {arXiv: Computation and Language},
  year    = {2020}
}

@book{moore2013development,
  title     = {The development of commonsense psychology},
  author    = {Moore, Chris},
  year      = {2013},
  publisher = {Psychology Press}
}

@article{lv2019graph,
  title   = {Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering},
  author  = {Shangwen Lv and Daya Guo and Jingjing Xu and Duyu Tang and Nan Duan and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Songlin Hu},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/1909.05311}
}

@inproceedings{Luong2015EffectiveAT,
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  author    = {Luong, Thang  and
	Pham, Hieu  and
	Manning, Christopher D.},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = sep,
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {1412--1421}
}

@inproceedings{conneau2018xnli,
  author    = {Conneau, Alexis
        and Rinott, Ruty
        and Lample, Guillaume
        and Williams, Adina
        and Bowman, Samuel R.
        and Schwenk, Holger
        and Stoyanov, Veselin},
  title     = {XNLI: Evaluating Cross-lingual Sentence Representations},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods
               in Natural Language Processing},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  location  = {Brussels, Belgium}
}

@article{ponti2020xcopa,
  title   = {XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning},
  author  = {Ponti, Edoardo Maria and Glava{\v{s}}, Goran and Majewska, Olga and Liu, Qianchu and Vuli{\'c}, Ivan and Korhonen, Anna},
  journal = {arXiv preprint arXiv:2005.00333},
  year    = {2020}
}


@article{conneau2019unsupervised,
  title   = {Unsupervised cross-lingual representation learning at scale},
  author  = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal = {arXiv preprint arXiv:1911.02116},
  year    = {2019}
}

@inproceedings{Sharma2018ConceptualCA,
  title     = {Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning},
  author    = {Sharma, Piyush  and
	Ding, Nan  and
	Goodman, Sebastian  and
	Soricut, Radu},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  pages     = {2556--2565},
  abstract  = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.}
}

@inproceedings{asai2020learning,
  title     = {Learning to Retrieve Reasoning Paths over Wikipedia Graph for  Question Answering},
  author    = {Akari Asai and Kazuma Hashimoto and Hannaneh Hajishirzi and Richard Socher and Caiming Xiong},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@article{guu2020realm,
  title   = {Realm: Retrieval-augmented language model pre-training},
  author  = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  journal = {arXiv preprint arXiv:2002.08909},
  year    = {2020}
}

@article{roberts2020much,
  title   = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
  author  = {Roberts, Adam and Raffel, Colin and Shazeer, Noam},
  journal = {arXiv preprint arXiv:2002.08910},
  year    = {2020}
}

@article{clark2018think,
  title   = {Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author  = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal = {arXiv preprint arXiv:1803.05457},
  year    = {2018}
}

@article{mihaylov2018can,
  title   = {Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author  = {Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal = {arXiv preprint arXiv:1809.02789},
  year    = {2018}
}

@inproceedings{khot2020qasc,
  title     = {QASC: A Dataset for Question Answering via Sentence Composition.},
  author    = {Khot, Tushar and Clark, Peter and Guerquin, Michal and Jansen, Peter and Sabharwal, Ashish},
  booktitle = {AAAI},
  pages     = {8082--8090},
  year      = {2020}
}

@article{salton1988term,
  title     = {Term-weighting approaches in automatic text retrieval},
  author    = {Salton, Gerard and Buckley, Christopher},
  journal   = {Information processing \& management},
  volume    = {24},
  number    = {5},
  pages     = {513--523},
  year      = {1988},
  publisher = {Elsevier}
}

@article{bhakthavatsalam2020genericskb,
  title   = {GenericsKB: A Knowledge Base of Generic Statements},
  author  = {Bhakthavatsalam, Sumithra and Anastasiades, Chloe and Clark, Peter},
  journal = {arXiv preprint arXiv:2005.00660},
  year    = {2020}
}

@article{Zhang2015DiscriminativeSW,
  title   = {Discriminative Syntax-Based Word Ordering for Text Generation},
  author  = {Yue Zhang and Stephen Clark},
  journal = {Computational Linguistics},
  year    = {2015},
  volume  = {41},
  pages   = {503-538}
}

@misc{chomsky1965aspects,
  title  = {Aspects of the Theory of Syntax},
  author = {Chomsky, Noam},
  year   = {1965}
}

@inproceedings{lake2018generalization,
  title     = {Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author    = {Lake, Brenden M and Baroni, Marco},
  journal   = {arXiv preprint arXiv:1711.00350},
  booktitle = {~},
  year      = {2017}
}

@inproceedings{keysers2020measuring,
  title     = {Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
  author    = {Daniel Keysers and Nathanael Sch{\"a}rli and Nathan Scales and Hylke Buisman and Daniel Furrer and Sergii Kashubin and Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and Olivier Bousquet},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@article{tincoff1999some,
  title     = {Some beginnings of word comprehension in 6-month-olds},
  author    = {Tincoff, Ruth and Jusczyk, Peter W},
  journal   = {Psychological science},
  volume    = {10},
  number    = {2},
  pages     = {172--175},
  year      = {1999},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}


@inproceedings{geva-etal-2019-modeling,
  title     = {Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets},
  author    = {Geva, Mor  and
	Goldberg, Yoav  and
	Berant, Jonathan},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  pages     = {1161--1166},
  abstract  = {Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.}
} 
@article{DBLP:journals/corr/abs-1904-01172,
  title   = {Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches},
  author  = {Shane Storks and Qiaozi Gao and Joyce Yue Chai},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1904.01172}
}

@inproceedings{Sutskever2014SequenceTS,
  title     = {Sequence to Sequence Learning with Neural Networks},
  author    = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  booktitle = {NIPS},
  year      = {2014}
}
@article{Trinh2018ASM,
  title   = {A Simple Method for Commonsense Reasoning},
  author  = {Trieu H. Trinh and Quoc V. Le},
  journal = {CoRR},
  year    = {2018},
  volume  = {abs/1806.02847}
}

@inproceedings{gu-etal-2016-incorporating,
  title     = {Incorporating Copying Mechanism in Sequence-to-Sequence Learning},
  author    = {Gu, Jiatao  and
	Lu, Zhengdong  and
	Li, Hang  and
	Li, Victor O.K.},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {1631--1640}
}

@inproceedings{Hokamp2017LexicallyCD,
  title     = {Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search},
  author    = {Hokamp, Chris  and
	Liu, Qun},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1535--1546},
  abstract  = {We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model{'}s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.}
}

@inproceedings{Hu2017TowardCG,
  title        = {Toward controlled generation of text},
  author       = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages        = {1587--1596},
  year         = {2017},
  organization = {JMLR. org}
}

@article{Stern2019InsertionTF,
  title   = {Insertion transformer: Flexible sequence generation via insertion operations},
  author  = {Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  journal = {arXiv preprint arXiv:1902.03249},
  year    = {2019}
}

@inproceedings{Devlin2019BERTPO,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{wang2018glue,
  title   = {Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author  = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal = {arXiv preprint arXiv:1804.07461},
  year    = {2018}
}

@article{kwiatkowski2019natural,
  title     = {Natural questions: a benchmark for question answering research},
  author    = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {7},
  pages     = {453--466},
  year      = {2019},
  publisher = {MIT Press}
}

@inproceedings{bisk2020piqa,
  title     = {PIQA: Reasoning about Physical Commonsense in Natural Language.},
  author    = {Bisk, Yonatan and Zellers, Rowan and LeBras, Ronan and Gao, Jianfeng and Choi, Yejin},
  booktitle = {AAAI},
  pages     = {7432--7439},
  year      = {2020}
}

@inproceedings{wang2019superglue,
  title     = {Superglue: A stickier benchmark for general-purpose language understanding systems},
  author    = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {3266--3280},
  year      = {2019}
}

@inproceedings{Lee2018DeterministicNN,
  title     = {Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement},
  author    = {Lee, Jason  and
	Mansimov, Elman  and
	Cho, Kyunghyun},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  month     = oct # {-} # nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  pages     = {1173--1182},
  abstract  = {We propose a conditional non-autoregressive neural sequence model based on iterative refinement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En-De and En-Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.}
}

@inproceedings{Vaswani2017AttentionIA,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}

@inproceedings{Gu2019LevenshteinT,
  title     = {Levenshtein transformer},
  author    = {Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {11179--11189},
  year      = {2019}
}

@inproceedings{Wang2019DoesIM,
  title     = {Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation},
  author    = {Cunxiang Wang and Shuailong Liang and Yue Zhang and Xiaonan Li and Tian Gao},
  booktitle = {ACL},
  year      = {2019}
}

@inproceedings{Mihaylov2018CanAS,
  title     = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author    = {Tzvetan Mihaylov and Peter F. Clark and Tushar Khot and Ashish Sabharwal},
  booktitle = {EMNLP},
  year      = {2018}
}

@inproceedings{Zellers2019HellaSwagCA,
  title     = {{H}ella{S}wag: Can a Machine Really Finish Your Sentence?},
  author    = {Zellers, Rowan  and
	Holtzman, Ari  and
	Bisk, Yonatan  and
	Farhadi, Ali  and
	Choi, Yejin},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {4791--4800},
  abstract  = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as {``}A woman sits at a piano,{''} a machine must select the most likely followup: {``}She sets her fingers on the keys.{''} With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textgreater}95{\%} accuracy), state-of-the-art models struggle ({\textless}48{\%}). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical {`}Goldilocks{'} zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.}
}


@inproceedings{kagnet-emnlp19,
  title     = {{K}ag{N}et: Knowledge-Aware Graph Networks for Commonsense Reasoning},
  author    = {Lin, Bill Yuchen  and
	Chen, Xinyue  and
	Chen, Jamin  and
	Ren, Xiang},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  pages     = {2829--2839},
  abstract  = {Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.}
}

@inproceedings{Li2016CommonsenseKB,
  title     = {Commonsense Knowledge Base Completion},
  author    = {Li, Xiang  and
	Taheri, Aynaz  and
	Tu, Lifu  and
	Gimpel, Kevin},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {1445--1455}
}

@inproceedings{Xu2018AutomaticEO,
  title     = {Automatic Extraction of Commonsense {L}ocated{N}ear Knowledge},
  author    = {Xu, Frank F.  and
	Lin, Bill Yuchen  and
	Zhu, Kenny},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  pages     = {96--101},
  abstract  = {LocatedNear relation is a kind of commonsense knowledge describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus. Also, we release two benchmark datasets for evaluation and future research.}
}

@inproceedings{huang-etal-2019-cosmos,
  title     = {Cosmos {QA}: Machine Reading Comprehension with Contextual Commonsense Reasoning},
  author    = {Huang, Lifu  and
	Le Bras, Ronan  and
	Bhagavatula, Chandra  and
	Choi, Yejin},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  pages     = {2391--2401},
  abstract  = {Understanding narratives requires reading between the lines, which in turn, requires interpreting the likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset of 35,600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. In stark contrast to most existing reading comprehension datasets where the questions focus on factual and literal understanding of the context paragraph, our dataset focuses on reading between the lines over a diverse collection of people{'}s everyday narratives, asking such questions as {``}what might be the possible reason of ...?'', or {``}what would have happened if ...'' that require reasoning beyond the exact text spans in the context. To establish baseline performances on Cosmos QA, we experiment with several state-of-the-art neural architectures for reading comprehension, and also propose a new architecture that improves over the competitive baselines. Experimental results demonstrate a significant gap between machine (68.4{\%}) and human performance (94{\%}), pointing to avenues for future research on commonsense machine comprehension. Dataset, code and leaderboard is publicly available at https://wilburone.github.io/cosmos.}
}

@article{Chen2019CODAHAA,
  title   = {CODAH: An Adversarially Authored Question-Answer Dataset for Common Sense},
  author  = {Michael Chen and Mike D'Arcy and Alisa Liu and Jared Fernandez and Doug Downey},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1904.04365}
}

@inproceedings{Lai2017RACELR,
  title     = {RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author    = {Guokun Lai and Qizhe Xie and Hanxiao Liu and Yiming Yang and Eduard H. Hovy},
  booktitle = {EMNLP},
  year      = {2017}
}

@inproceedings{Klein2017OpenNMTOT,
  title     = {{O}pen{NMT}: Open-Source Toolkit for Neural Machine Translation},
  author    = {Klein, Guillaume  and
	Kim, Yoon  and
	Deng, Yuntian  and
	Senellart, Jean  and
	Rush, Alexander},
  booktitle = {Proceedings of {ACL} 2017, System Demonstrations},
  month     = jul,
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {67--72}
}

@inproceedings{Rajpurkar2016SQuAD10,
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author    = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle = {EMNLP},
  year      = {2016}
}


@article{Liu2019RoBERTaAR,
  title   = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author  = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1907.11692}
}
@article{young-etal-2014-image,
  title     = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author    = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {2},
  pages     = {67--78},
  year      = {2014},
  publisher = {MIT Press}
}

@article{t5,
  title   = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author  = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal = {Journal of Machine Learning Research},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  year    = {2020}
}


@article{flickrentitiesijcv,
  author  = {Bryan A. Plummer and Liwei Wang and Christopher M. Cervantes and Juan C. Caicedo and Julia Hockenmaier and Svetlana Lazebnik},
  title   = {Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models},
  journal = {IJCV},
  volume  = {123},
  number  = {1},
  pages   = {74-93},
  year    = {2017}
}

@article{bart,
  title   = {Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author  = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:1910.13461},
  year    = {2019}
}

@article{dpr,
  title   = {Dense Passage Retrieval for Open-Domain Question Answering},
  author  = {Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal = {arXiv preprint arXiv:2004.04906},
  year    = {2020}
}


@inproceedings{drkit,
  title     = {Differentiable Reasoning over a Virtual Knowledge Base},
  author    = {Bhuwan Dhingra and Manzil Zaheer and Vidhisha Balachandran and Graham Neubig and Ruslan Salakhutdinov and William W. Cohen},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}



@inproceedings{krishna2017dense,
  title     = {Dense-captioning events in videos},
  author    = {Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {706--715},
  year      = {2017}
}

@article{lsmdc,
  title     = {Movie description},
  author    = {Rohrbach, Anna and Torabi, Atousa and Rohrbach, Marcus and Tandon, Niket and Pal, Christopher and Larochelle, Hugo and Courville, Aaron and Schiele, Bernt},
  journal   = {International Journal of Computer Vision},
  volume    = {123},
  number    = {1},
  pages     = {94--120},
  year      = {2017},
  publisher = {Springer}
}

@inproceedings{Speer2017ConceptNet5A,
  title     = {Conceptnet 5.5: An open multilingual graph of general knowledge},
  author    = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle = {Thirty-First AAAI Conference on Artificial Intelligence},
  year      = {2017}
}

@article{trinh2019do,
  title   = {Do Language Models Have Common Sense?},
  author  = {Trieu H. Trinh and Quoc V. Le},
  year    = {2019},
  journal = {OpenReview},
  volume  = {ICLR submissions}
}

@article{feng2020scalable,
  title   = {Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering},
  author  = {Feng, Yanlin and Chen, Xinyue and Lin, Bill Yuchen and Wang, Peifeng and Yan, Jun and Ren, Xiang},
  journal = {arXiv preprint arXiv:2005.00646},
  year    = {2020}
}

@article{khashabi2020unifiedqa,
  title   = {Unifiedqa: Crossing format boundaries with a single qa system},
  author  = {Khashabi, Daniel and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  journal = {arXiv preprint arXiv:2005.00700},
  year    = {2020}
}

@article{lin2020birds,
  title   = {Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models},
  author  = {Lin, Bill Yuchen and Lee, Seyeon and Khanna, Rahul and Ren, Xiang},
  journal = {arXiv preprint arXiv:2005.00683},
  year    = {2020}
}

@techreport{Hu2020,
  abstract        = {Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models , a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark , a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark 1 to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.},
  author          = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  file            = {:C\:/Users/yuchenlin/OneDrive - University of Southern California/papers/XTREME A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization - 2020 - Hu et al.pdf:pdf},
  issn            = {2640-3498},
  mendeley-groups = {Multilingual,general NLP},
  month           = {nov},
  pages           = {4411--4421},
  publisher       = {PMLR},
  title           = {{XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization}},
  url             = {https://sites.},
  year            = {2020}
}


@article{jiang2020can,
  title     = {How can we know what language models know?},
  author    = {Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {8},
  pages     = {423--438},
  year      = {2020},
  publisher = {MIT Press}
}


@article{lin2019commongen,
  title   = {Common{G}en: A constrained text generation challenge for generative commonsense reasoning},
  author  = {Lin, Bill Yuchen and Shen, Ming and Xing, Yu and Zhou, Pei and Ren, Xiang},
  journal = {arXiv preprint arXiv:1911.03705},
  year    = {2019}
}

@inproceedings{Clark2020ELECTRA,
  title     = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author    = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@inproceedings{Lan2020ALBERT,
  title     = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author    = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@inproceedings{snli:emnlp2015,
  author    = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  publisher = {Association for Computational Linguistics},
  title     = {A large annotated corpus for learning natural language inference},
  year      = {2015}
}



@inproceedings{Wang_2019_ICCV,
  title     = {VATEX: A large-scale, high-quality multilingual dataset for video-and-language research},
  author    = {Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages     = {4581--4591},
  year      = {2019}
}


@inproceedings{Annervaz2018LearningBD,
  title     = {Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing},
  author    = {K. M. Annervaz and Somnath Basu Roy Chowdhury and Ambedkar Dukkipati},
  booktitle = {NAACL-HLT},
  year      = {2018}
}
@article{johnson1980mental,
  title     = {Mental models in cognitive science},
  author    = {Johnson-Laird, Philip N},
  journal   = {Cognitive science},
  volume    = {4},
  number    = {1},
  pages     = {71--115},
  year      = {1980},
  publisher = {Elsevier}
} 
@inproceedings{Hudson2018CompositionalAN,
  title     = {Compositional Attention Networks for Machine Reasoning},
  author    = {Drew A. Hudson and Christopher D. Manning},
  booktitle = {ICLR},
  year      = {2018}
}
@inproceedings{kipf2016semi,
  title     = {Semi-Supervised Classification with Graph Convolutional Networks},
  author    = {Kipf, Thomas N and Welling, Max},
  booktitle = {Proceedings of ICLR},
  year      = {2017}
}
@inproceedings{Wang2019ImprovingNL,
  title     = {Improving Natural Language Inference Using External Knowledge in the Science Questions Domain},
  author    = {Xiaoyan Wang and Pavan Kapanipathi and Ryan Musa and Mo Yu and Kartik Talamadupula and Ibrahim Abdelaziz and Maria Chang and Achille Fokoue and Bassem Makni and Nicholas Mattei and Michael Witbrock},
  booktitle = {AAAI},
  year      = {2019}
}


@inproceedings{puduppully-etal-2017-transition,
  title     = {Transition-Based Deep Input Linearization},
  author    = {Puduppully, Ratish  and
	Zhang, Yue  and
	Shrivastava, Manish},
  booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  month     = apr,
  year      = {2017},
  address   = {Valencia, Spain},
  publisher = {Association for Computational Linguistics},
  pages     = {643--654},
  abstract  = {Traditional methods for deep NLG adopt pipeline approaches comprising stages such as constructing syntactic input, predicting function words, linearizing the syntactic input and generating the surface forms. Though easier to visualize, pipeline approaches suffer from error propagation. In addition, information available across modules cannot be leveraged by all modules. We construct a transition-based model to jointly perform linearization, function word prediction and morphological generation, which considerably improves upon the accuracy compared to a pipelined baseline system. On a standard deep input linearization shared task, our system achieves the best results reported so far.}
}

@inproceedings{wang-etal-2020-semeval,
  title     = {{S}em{E}val-2020 Task 4: Commonsense Validation and Explanation},
  author    = {Wang, Cunxiang  and
	Liang, Shuailong  and
	Jin, Yili  and
	Wang, Yilong  and
	Zhu, Xiaodan  and
	Zhang, Yue},
  booktitle = {Proceedings of The 14th International Workshop on Semantic Evaluation},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{wang-etal-2019-make,
  title     = {Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation},
  author    = {Wang, Cunxiang  and
	Liang, Shuailong  and
	Zhang, Yue  and
	Li, Xiaonan  and
	Gao, Tian},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {4020--4026},
  abstract  = {Introducing common sense to natural language understanding systems has received increasing research attention. It remains a fundamental question on how to evaluate whether a system has the sense-making capability. Existing benchmarks measure common sense knowledge indirectly or without reasoning. In this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense-making.}
}

@article{Yang2019XLNetGA,
  title   = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author  = {Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1906.08237}
}

@article{Wolf2019HuggingFacesTS,
  title   = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author  = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1910.03771}
}

@inproceedings{post-vilar-2018-fast,
  title     = {Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation},
  author    = {Post, Matt  and
	Vilar, David},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {1314--1324}
}


@inproceedings{Zhang2020BERTScore,
  title     = {"BERTScore: Evaluating Text Generation with BERT"},
  author    = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@inproceedings{Rajani2019ExplainYL,
  title     = {Explain Yourself! Leveraging Language Models for Commonsense Reasoning},
  author    = {Nazneen Fatema Rajani and Bryan McCann and Caiming Xiong and Richard Socher},
  booktitle = {ACL},
  year      = {2019}
}

@inproceedings{Schlichtkrull2018ModelingRD,
  title     = {Modeling Relational Data with Graph Convolutional Networks},
  author    = {Michael Sejr Schlichtkrull and Thomas N. Kipf and Peter Bloem and Rianne van den Berg and Ivan Titov and Max Welling},
  booktitle = {European Semantic Web Conference},
  year      = {2018}
}
@article{Hochreiter1997LongSM,
  title   = {Long Short-Term Memory},
  author  = {Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal = {Neural Computation},
  year    = {1997},
  volume  = {9},
  pages   = {1735-1780}
}
@inproceedings{Mihaylov2018KnowledgeableRE,
  title     = {Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge},
  author    = {Todor Mihaylov and Anette Frank},
  booktitle = {ACL},
  year      = {2018}
}

@inproceedings{Yang2017LeveragingKB,
  title     = {Leveraging Knowledge Bases in LSTMs for Improving Machine Reading},
  author    = {Bishan Yang and Tom Michael Mitchell},
  booktitle = {ACL},
  year      = {2017}
}

@inproceedings{Talmor2018CommonsenseQAAQ,
  title     = {{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge},
  author    = {Talmor, Alon  and
	Herzig, Jonathan  and
	Lourie, Nicholas  and
	Berant, Jonathan},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  pages     = {4149--4158},
  abstract  = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.}
}

@inproceedings{sap-etal-2019-social,
  title     = {Social {IQ}a: Commonsense Reasoning about Social Interactions},
  author    = {Sap, Maarten  and
	Rashkin, Hannah  and
	Chen, Derek  and
	Le Bras, Ronan  and
	Choi, Yejin},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  pages     = {4463--4473},
  abstract  = {We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).}
}

@inproceedings{Fan2018HierarchicalNS,
  title     = {Hierarchical Neural Story Generation},
  author    = {Fan, Angela  and
	Lewis, Mike  and
	Dauphin, Yann},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  pages     = {889--898},
  abstract  = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.}
}

@inproceedings{Zellers2019FromRT,
  title     = {From recognition to cognition: Visual commonsense reasoning},
  author    = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {6720--6731},
  year      = {2019}
}

@inproceedings{Yang2019KnowledgeableSA,
  title     = {Knowledgeable storyteller: a commonsense-driven generative model for visual storytelling},
  author    = {Yang, Pengcheng and Luo, Fuli and Chen, Peng and Li, Lei and Yin, Zhiyi and He, Xiaodong and Sun, Xu},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI},
  pages     = {5356--5362},
  year      = {2019}
}

@inproceedings{LaiXLYH17,
  author    = {Guokun Lai and
	Qizhe Xie and
	Hanxiao Liu and
	Yiming Yang and
	Eduard H. Hovy},
  title     = {{RACE:} Large-scale ReAding Comprehension Dataset From Examinations},
  booktitle = {EMNLP},
  year      = {2017}
}

@inproceedings{yang2018hotpotqa,
  title     = {{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author    = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
  booktitle = {EMNLP},
  year      = {2018}
}

@inproceedings{Zellers2018SWAGAL,
  title     = {{SWAG}: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference},
  author    = {Zellers, Rowan  and
	Bisk, Yonatan  and
	Schwartz, Roy  and
	Choi, Yejin},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  month     = oct # {-} # nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  pages     = {93--104},
  abstract  = {Given a partial description like {``}she opened the hood of the car,{''} humans can reason about the situation and anticipate what might come next ({''}then, she examined the engine{''}). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88{\%}), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.}
}

@inproceedings{Pascanu2014HowTC,
  title   = {How to Construct Deep Recurrent Neural Networks},
  author  = {Razvan Pascanu and Çaglar G{\"u}lçehre and Kyunghyun Cho and Yoshua Bengio},
  journal = {ICLR},
  year    = {2014}
}  
@inproceedings{Dong2019UnifiedLM,
  title     = {Unified language model pre-training for natural language understanding and generation},
  author    = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {13042--13054},
  year      = {2019}
}

@inproceedings{Lin2004ROUGEAP,
  title     = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  author    = {Lin, Chin-Yew},
  booktitle = {Text Summarization Branches Out},
  month     = jul,
  year      = {2004},
  address   = {Barcelona, Spain},
  publisher = {Association for Computational Linguistics},
  pages     = {74--81}
}

@inproceedings{Anderson2016SPICESP,
  title        = {Spice: Semantic propositional image caption evaluation},
  author       = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle    = {European Conference on Computer Vision},
  pages        = {382--398},
  year         = {2016},
  organization = {Springer}
}

@inproceedings{Vedantam2014CIDErCI,
  title     = {Cider: Consensus-based image description evaluation},
  author    = {Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4566--4575},
  year      = {2015}
}

@inproceedings{Papineni2001BleuAM,
  title     = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
  author    = {Papineni, Kishore  and
	Roukos, Salim  and
	Ward, Todd  and
	Zhu, Wei-Jing},
  booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2002},
  address   = {Philadelphia, Pennsylvania, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {311--318}
}



@inproceedings{Miao2018CGMHCS,
  title     = {Cgmh: Constrained sentence generation by metropolis-hastings sampling},
  author    = {Miao, Ning and Zhou, Hao and Mou, Lili and Yan, Rui and Li, Lei},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {33},
  pages     = {6834--6842},
  year      = {2019}
}

@inproceedings{See2017GetTT,
  title     = {Get To The Point: Summarization with Pointer-Generator Networks},
  author    = {Abigail See and Peter J. Liu and Christopher D. Manning},
  booktitle = {ACL},
  year      = {2017}
}


@book{arbib1987schema,
  title     = {From schema theory to language.},
  author    = {Arbib, Michael A and Conklin, E Jeffrey and Hill, Jane C},
  year      = {1987},
  publisher = {Oxford University Press}
}

@article{axelrod1973schema,
  title     = {Schema theory: An information processing model of perception and cognition},
  author    = {Axelrod, Robert},
  journal   = {American political science review},
  volume    = {67},
  number    = {4},
  pages     = {1248--1266},
  year      = {1973},
  publisher = {Cambridge University Press}
}

@article{arbib1992schema,
  title     = {Schema theory},
  author    = {Arbib, Michael A},
  journal   = {The Encyclopedia of Artificial Intelligence},
  volume    = {2},
  pages     = {1427--1443},
  year      = {1992},
  publisher = {Wiley-Interscience}
}

@article{anderson1984schema,
  title   = {A schema-theoretic view of basic processes in reading comprehension},
  author  = {Anderson, Richard C and Pearson, P David},
  journal = {Handbook of reading research},
  volume  = {1},
  pages   = {255--291},
  year    = {1984}
}



@inproceedings{DBLP:conf/cvpr/LuYBP18,
  author    = {Jiasen Lu and
	Jianwei Yang and
	Dhruv Batra and
	Devi Parikh},
  title     = {Neural Baby Talk},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
	{CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {7219--7228},
  publisher = {{IEEE} Computer Society},
  year      = {2018},
  doi       = {10.1109/CVPR.2018.00754},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{j-kurisinkel-chen-2019-set,
  title     = {Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes},
  author    = {J Kurisinkel, Litton  and
	Chen, Nancy},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  pages     = {6165--6175},
  abstract  = {We present set to ordered text, a natural language generation task applied to automatically generating discharge instructions from admission ICD (International Classification of Diseases) codes. This task differs from other natural language generation tasks in the following ways: (1) The input is a set of identifiable entities (ICD codes) where the relations between individual entity are not explicitly specified. (2) The output text is not a narrative description (e.g. news articles) composed from the input. Rather, inferences are made from the input (symptoms specified in ICD codes) to generate the output (instructions). (3) There is an optimal order in which each sentence (instruction) should appear in the output. Unlike most other tasks, neither the input (ICD codes) nor their corresponding symptoms appear in the output, so the ordering of the output instructions needs to be learned in an unsupervised fashion. Based on clinical intuition, we hypothesize that each instruction in the output is mapped to a subset of ICD codes specified in the input. We propose a neural architecture that jointly models (a) subset selection: choosing relevant subsets from a set of input entities; (b) content ordering: learning the order of instructions; and (c) text generation: representing the instructions corresponding to the selected subsets in natural language. In addition, we penalize redundancy during beam search to improve tractability for long text generation. Our model outperforms baseline models in BLEU scores and human evaluation. We plan to extend this work to other tasks such as recipe generation from ingredients.}
}
@inproceedings{Devlin2019,
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {NAACL-HLT},
  year      = {2019}
}

@inproceedings{Zhang2019ConversationGW,
  title     = {Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs},
  author    = {Houyu Zhang and Zhenghao Liu and Chenyan Xiong and Zhiyuan Liu},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  note      = {to appear},
  year      = {2020}
}



@inproceedings{Susanto2020LexicallyCN,
  title     = {Lexically Constrained Neural Machine Translation with Levenshtein Transformer},
  author    = {Raymond Hendy Susanto and Shamil Chollampatt and Li-ling Tan},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  note      = {to appear},
  year      = {2020}
}


@article{petroni2019language,
  title   = {Language models as knowledge bases?},
  author  = {Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal = {arXiv preprint arXiv:1909.01066},
  year    = {2019}
}

@misc{radford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019}
}

@article{radford2018improving,
  title  = {Improving language understanding by generative pre-training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year   = {2018}
}

@article{zhou2018graph,
  title   = {Graph Neural Networks: A Review of Methods and Applications},
  author  = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
  journal = {arXiv preprint arXiv:1812.08434},
  year    = {2018}
}

@article{jaume2018image,
  title   = {Image-Level Attentional Context Modeling Using Nested-Graph Neural Networks},
  author  = {Jaume, Guillaume and Bozorgtabar, Behzad and Ekenel, Hazim Kemal and Thiran, Jean-Philippe and Gabrani, Maria},
  journal = {arXiv preprint arXiv:1811.03830},
  year    = {2018}
}

@article{Wang2018ImprovingNL,
  title     = {Improving Natural Language Inference Using External Knowledge in the Science Questions Domain},
  author    = {Xiaoyan Wang and Pavan Kapanipathi and Ryan Musa and Mo Yu and Kartik Talamadupula and Ibrahim Abdelaziz and Maria Chang and Achille Fokoue and Bassem Makni and Nicholas Mattei and Michael Witbrock},
  booktitle = {AAAI},
  year      = {2019}
}

@inproceedings{Khashabi2017LearningWI,
  title     = {Learning What is Essential in Questions},
  author    = {Daniel Khashabi and Tushar Khot and Ashutosh Sabharwal and Dan Roth},
  booktitle = {CoNLL},
  year      = {2017}
}


@article{garey1977rectilinear,
  title     = {The rectilinear Steiner tree problem is NP-complete},
  author    = {Garey, Michael R and Johnson, David S.},
  journal   = {SIAM Journal on Applied Mathematics},
  volume    = {32},
  number    = {4},
  pages     = {826--834},
  year      = {1977},
  publisher = {SIAM}
}


@article{Zhong2018ImprovingQA,
  title   = {Improving Question Answering by Commonsense-Based Pre-Training},
  author  = {Wanjun Zhong and Duyu Tang and Nan Duan and Ming Zhou and Jiahai Wang and Jian Yin},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1809.03568}
}

@inproceedings{Wang2014KnowledgeGE,
  title     = {Knowledge Graph Embedding by Translating on Hyperplanes},
  author    = {Zhen Wang and Jianwen Zhang and Jianlin Feng and Zheng Chen},
  booktitle = {AAAI},
  year      = {2014}
}

@inproceedings{Singh2018SemanticallyEA,
  title     = {Semantically Equivalent Adversarial Rules for Debugging NLP models},
  author    = {Sameer Singh and Carlos Guestrin and Marco T{\'u}lio Ribeiro},
  booktitle = {ACL},
  year      = {2018}
}

@article{weissenborn2017dynamic,
  title   = {Dynamic integration of background knowledge in neural NLU systems},
  author  = {Weissenborn, Dirk and Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Dyer, Chris},
  journal = {arXiv preprint arXiv:1706.02596},
  year    = {2017}
} 

@inproceedings{Kingma2015AdamAM,
  title     = {Adam: A Method for Stochastic Optimization},
  author    = {Diederik P. Kingma and Jimmy Ba},
  booktitle = {ICLR},
  year      = {2015}
}

@article{Battaglia2018RelationalIB,
  title   = {Relational inductive biases, deep learning, and graph networks},
  author  = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vin{\'i}cius Flores Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Çaglar G{\"u}lçehre and Francis Song and Andrew J. Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey R. Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matthew Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
  journal = {CoRR},
  year    = {2018},
  volume  = {abs/1806.01261}
}

@inproceedings{Yang2016HierarchicalAN,
  title     = {Hierarchical Attention Networks for Document Classification},
  author    = {Zichao Yang and Diyi Yang and Chris Dyer and Xiaodong He and Alexander J. Smola and Eduard H. Hovy},
  booktitle = {NAACL-HLT},
  year      = {2016}
}

@article{das2019multi,
  title   = {Multi-step retriever-reader interaction for scalable open-domain question answering},
  author  = {Das, Rajarshi and Dhuliawala, Shehzaad and Zaheer, Manzil and McCallum, Andrew},
  journal = {arXiv preprint arXiv:1905.05733},
  year    = {2019}
}

@article{lee2019latent,
  title   = {Latent retrieval for weakly supervised open domain question answering},
  author  = {Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1906.00300},
  year    = {2019}
}

@article{chen2017reading,
  title   = {Reading wikipedia to answer open-domain questions},
  author  = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  journal = {arXiv preprint arXiv:1704.00051},
  year    = {2017}
}

@inproceedings{li2016commonsense,
  title     = {Commonsense knowledge base completion},
  author    = {Li, Xiang and Taheri, Aynaz and Tu, Lifu and Gimpel, Kevin},
  booktitle = {ACL},
  year      = {2016}
}

@article{qi2019answering,
  title   = {Answering complex open-domain questions through iterative query generation},
  author  = {Qi, Peng and Lin, Xiaowen and Mehr, Leo and Wang, Zijian and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1910.07000},
  year    = {2019}
}

@article{sun2018open,
  title   = {Open domain question answering using early fusion of knowledge bases and text},
  author  = {Sun, Haitian and Dhingra, Bhuwan and Zaheer, Manzil and Mazaitis, Kathryn and Salakhutdinov, Ruslan and Cohen, William W},
  journal = {arXiv preprint arXiv:1809.00782},
  year    = {2018}
}

@inproceedings{feldman-el-yaniv-2019-multi,
  title     = {Multi-Hop Paragraph Retrieval for Open-Domain Question Answering},
  author    = {Feldman, Yair  and
      El-Yaniv, Ran},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {2296--2309},
  abstract  = {This paper is concerned with the task of multi-hop open-domain Question Answering (QA). This task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching. We present a method for retrieving multiple supporting paragraphs, nested amidst a large knowledge base, which contain the necessary evidence to answer a given question. Our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph. The retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source. Our method achieves state-of-the-art performance over two well-known datasets, SQuAD-Open and HotpotQA, which serve as our single- and multi-hop open-domain QA benchmarks, respectively.}
}

@article{sun2019pullnet,
  title   = {Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text},
  author  = {Sun, Haitian and Bedrax-Weiss, Tania and Cohen, William W},
  journal = {arXiv preprint arXiv:1904.09537},
  year    = {2019}
}

@article{lewis2020retrieval,
  title   = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author  = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandara and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal = {arXiv preprint arXiv:2005.11401},
  year    = {2020}
}

@article{raffel2019exploring,
  title   = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author  = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal = {arXiv preprint arXiv:1910.10683},
  year    = {2019}
}
 


@inproceedings{Banerjee2005METEORAA,
  title     = {{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments},
  author    = {Banerjee, Satanjeev  and
	Lavie, Alon},
  booktitle = {Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
  month     = jun,
  year      = {2005},
  address   = {Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {65--72}
}

@article{lewis2019bart,
  title   = {Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author  = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:1910.13461},
  year    = {2019}
}

@inproceedings{lv2020graph,
  title     = {Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering.},
  author    = {Lv, Shangwen and Guo, Daya and Xu, Jingjing and Tang, Duyu and Duan, Nan and Gong, Ming and Shou, Linjun and Jiang, Daxin and Cao, Guihong and Hu, Songlin},
  booktitle = {AAAI},
  pages     = {8449--8456},
  year      = {2020}
}


@article{johnson2019billion,
  title     = {Billion-scale similarity search with GPUs},
  author    = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal   = {IEEE Transactions on Big Data},
  year      = {2019},
  publisher = {IEEE}
}

@inproceedings{Hasler2018NeuralMT,
  title     = {Neural Machine Translation Decoding with Terminology Constraints},
  author    = {Hasler, Eva  and
	de Gispert, Adri{\`a}  and
	Iglesias, Gonzalo  and
	Byrne, Bill},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {506--512},
  abstract  = {Despite the impressive quality improvements yielded by neural machine translation (NMT) systems, controlling their translation output to adhere to user-provided terminology constraints remains an open problem. We describe our approach to constrained neural decoding based on finite-state machines and multi-stack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints.}
}
@inproceedings{Dinu2019TrainingNM,
  title     = {Training Neural Machine Translation to Apply Terminology Constraints},
  author    = {Dinu, Georgiana  and
	Mathur, Prashant  and
	Federico, Marcello  and
	Al-Onaizan, Yaser},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {3063--3068},
  abstract  = {This paper proposes a novel method to inject custom terminology into neural machine translation at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.}
}

@inproceedings{Li2018DeleteRG,
  title     = {Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer},
  author    = {Li, Juncen  and
	Jia, Robin  and
	He, He  and
	Liang, Percy},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {1865--1874},
  abstract  = {We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., {``}screen is just the right size{''} to {``}screen is too small{''}). Our training data includes only sentences labeled with their attribute (e.g., positive and negative), but not pairs of sentences that only differ in the attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., {``}too small{''}). Our strongest method extracts content words by deleting phrases associated with the sentence{'}s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. Based on human evaluation, our best method generates grammatical and appropriate responses on 22{\%} more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.}
}

@inproceedings{Lin2018MiningCD,
  title     = {Mining Cross-Cultural Differences and Similarities in Social Media},
  author    = {Lin, Bill Yuchen  and
	Xu, Frank F.  and
	Zhu, Kenny  and
	Hwang, Seung-won},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  pages     = {709--719},
  abstract  = {Cross-cultural differences and similarities are common in cross-lingual natural language understanding, especially for research in social media. For instance, people of distinct cultures often hold different opinions on a single named entity. Also, understanding slang terms across languages requires knowledge of cross-cultural similarities. In this paper, we study the problem of computing such cross-cultural differences and similarities. We present a lightweight yet effective approach, and evaluate it on two novel tasks: 1) mining cross-cultural differences of named entities and 2) finding similar terms for slang across languages. Experimental results show that our framework substantially outperforms a number of baseline methods on both tasks. The framework could be useful for machine translation applications and research in computational social science.}
}

@article{seo2019real,
  title   = {Real-time open-domain question answering with dense-sparse phrase index},
  author  = {Seo, Minjoon and Lee, Jinhyuk and Kwiatkowski, Tom and Parikh, Ankur P and Farhadi, Ali and Hajishirzi, Hannaneh},
  journal = {arXiv preprint arXiv:1906.05807},
  year    = {2019}
}

@article{cohen2019neural,
  title   = {Neural query language: A knowledge base query language for tensorflow},
  author  = {Cohen, William W and Siegler, Matthew and Hofer, Alex},
  journal = {arXiv preprint arXiv:1905.06209},
  year    = {2019}
}

@inproceedings{Cohen2020Scalable,
  title     = {Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},
  author    = {William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@article{Zhu2019TextI,
  title   = {Text Infilling},
  author  = {Wanrong Zhu and Zhiting Hu and Eric P. Xing},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1901.00158}
}

@article{Luo2019ADR,
  title   = {A dual reinforcement learning framework for unsupervised text style transfer},
  author  = {Luo, Fuli and Li, Peng and Zhou, Jie and Yang, Pengcheng and Chang, Baobao and Sui, Zhifang and Sun, Xu},
  journal = {arXiv preprint arXiv:1905.10060},
  year    = {2019}
}


@inproceedings{Luo2019TowardsFT,
  title     = {Towards Fine-grained Text Sentiment Transfer},
  author    = {Luo, Fuli  and
	Li, Peng  and
	Yang, Pengcheng  and
	Zhou, Jie  and
	Tan, Yutong  and
	Chang, Baobao  and
	Sui, Zhifang  and
	Sun, Xu},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {2013--2022},
  abstract  = {In this paper, we focus on the task of fine-grained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation.}
}

@inproceedings{fu2018style,
  title     = {Style transfer in text: Exploration and evaluation},
  author    = {Fu, Zhenxin and Tan, Xiaoye and Peng, Nanyun and Zhao, Dongyan and Yan, Rui},
  booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
  year      = {2018}
}

@inproceedings{Levesque2011TheWS,
  title     = {The Winograd Schema Challenge},
  author    = {Hector J. Levesque},
  booktitle = {AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},
  year      = {2011}
}


@inproceedings{sap2018atomic,
  title     = {Atomic: An atlas of machine commonsense for if-then reasoning},
  author    = {Sap, Maarten and Le Bras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A and Choi, Yejin},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {33},
  pages     = {3027--3035},
  year      = {2019}
}

@inproceedings{Tandon2017WebChild2,
  title     = {WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation},
  author    = {Niket Tandon and Gerard de Melo and Gerhard Weikum},
  booktitle = {ACL},
  year      = {2017}
}

@inproceedings{Santoro2017ASN,
  title     = {A simple neural network module for relational reasoning},
  author    = {Adam Santoro and David Raposo and David G. T. Barrett and Mateusz Malinowski and Razvan Pascanu and Peter W. Battaglia and Timothy P. Lillicrap},
  booktitle = {NIPS},
  year      = {2017}
}


@inproceedings{Marcheggiani2017EncodingSW,
  title     = {Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling},
  author    = {Diego Marcheggiani and Ivan Titov},
  booktitle = {EMNLP},
  year      = {2017}
}

@inproceedings{Bastings2017GraphCE,
  title     = {Graph Convolutional Encoders for Syntax-aware Neural Machine Translation},
  author    = {Joost Bastings and Ivan Titov and Wilker Aziz and Diego Marcheggiani and Khalil Sima'an},
  booktitle = {EMNLP},
  year      = {2017}
}

@inproceedings{Zhang2018GraphCO,
  title     = {Graph Convolution over Pruned Dependency Trees Improves Relation Extraction},
  author    = {Yuhao Zhang and Peng Qi and Christopher D. Manning},
  booktitle = {EMNLP},
  year      = {2018}
}

@book{Aho:72,
  author    = {Alfred V. Aho and Jeffrey D. Ullman},
  title     = {The Theory of Parsing, Translation and Compiling},
  year      = {1972},
  volume    = {1},
  publisher = {Prentice-Hall},
  address   = {Englewood Cliffs, NJ}
}

@book{APA:83,
  author    = {{American Psychological Association}},
  title     = {Publications Manual},
  year      = {1983},
  publisher = {American Psychological Association},
  address   = {Washington, DC}
}

@article{ACM:83,
  author  = {Association for Computing Machinery},
  year    = {1983},
  journal = {Computing Reviews},
  volume  = {24},
  number  = {11},
  pages   = {503--512}
}

@article{Chandra:81,
  author  = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
  year    = {1981},
  title   = {Alternation},
  journal = {Journal of the Association for Computing Machinery},
  volume  = {28},
  number  = {1},
  pages   = {114--133}
}

@inproceedings{andrew2007scalable,
  title     = {Scalable training of {L1}-regularized log-linear models},
  author    = {Andrew, Galen and Gao, Jianfeng},
  booktitle = {ICML},
  pages     = {33--40},
  year      = {2007}
}

@inproceedings{Zhou2017EmotionalCM,
  title     = {Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory},
  author    = {Hao Zhou and Minlie Huang and Tianyang Zhang and Xiaoyan Zhu and Bing Liu},
  booktitle = {AAAI},
  year      = {2017}
}

@article{Qiao2019MirrorGANLT,
  title   = {MirrorGAN: Learning Text-to-image Generation by Redescription},
  author  = {Tingting Qiao and Jing Zhang and Duanqing Xu and Dacheng Tao},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1903.05854}
}

@inproceedings{Hudson2019GQAAN,
  title     = {GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
  author    = {Drew A. Hudson and Christopher D. Manning},
  booktitle = {CVPR},
  year      = {2019}
}

@inproceedings{Zhou2018CommonsenseKA,
  title     = {Commonsense Knowledge Aware Conversation Generation with Graph Attention},
  author    = {Hao Zhou and Tom Young and Minlie Huang and Haizhou Zhao and Jingfang Xu and Xiaoyan Zhu},
  booktitle = {IJCAI},
  year      = {2018}
}

@inproceedings{Feng2018TopictoEssayGW,
  title     = {Topic-to-Essay Generation with Neural Networks.},
  author    = {Feng, Xiaocheng and Liu, Ming and Liu, Jiahao and Qin, Bing and Sun, Yibo and Liu, Ting},
  booktitle = {IJCAI},
  pages     = {4078--4084},
  year      = {2018}
}

@inproceedings{Yang2019EnhancingTG,
  title     = {Enhancing Topic-to-Essay Generation with External Commonsense Knowledge},
  author    = {Yang, Pengcheng  and
	Li, Lei  and
	Luo, Fuli  and
	Liu, Tianyu  and
	Sun, Xu},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {2002--2012},
  abstract  = {Automatic topic-to-essay generation is a challenging task since it requires generating novel, diverse, and topic-consistent paragraph-level text with a set of topics as input. Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge. However, this commonsense knowledge provides additional background information, which can help to generate essays that are more novel and diverse. Towards filling this gap, we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism. Besides, the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency. We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay. Experiments show that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent than existing methods in terms of both automatic and human evaluation.}
}
@inproceedings{Guan2018StoryEG,
  title     = {Story ending generation with incremental encoding and commonsense knowledge},
  author    = {Guan, Jian and Wang, Yansen and Huang, Minlie},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {33},
  pages     = {6473--6480},
  year      = {2019}
}
@book{Gusfield:97,
  author    = {Dan Gusfield},
  title     = {Algorithms on Strings, Trees and Sequences},
  year      = {1997},
  publisher = {Cambridge University Press},
  address   = {Cambridge, UK}
}

@inproceedings{borsch2011,
  address   = {Canberra, Australia},
  author    = {Benjamin Borschinger and Mark Johnson},
  booktitle = {the Australasian Language Technology Association Workshop},
  month     = {December},
  pages     = {10--18},
  title     = {A Particle Filter algorithm for {B}ayesian Wordsegmentation},
  year      = {2011}
}

@article{rasooli-tetrault-2015,
  author  = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
  title   = {Yara Parser: {A} Fast and Accurate Dependency Parser},
  journal = {Computing Research Repository},
  volume  = {arXiv:1503.06733},
  year    = {2015},
  note    = {version 2}
}
@inproceedings{Weissenborn2018DynamicIO,
  title  = {Dynamic Integration of Background Knowledge in Neural NLU Systems},
  author = {Dirk Weissenborn and Tom'avs Kovcisk'y and Chris Dyer},
  year   = {2018}
}

@inproceedings{bollacker2008freebase,
  title        = {Freebase: a collaboratively created graph database for structuring human knowledge},
  author       = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  booktitle    = {SIGKDD},
  pages        = {1247--1250},
  year         = {2008},
  organization = {AcM}
}
@article{shen2014entity,
  title     = {Entity linking with a knowledge base: Issues, techniques, and solutions},
  author    = {Shen, Wei and Wang, Jianyong and Han, Jiawei},
  journal   = {TKDE},
  volume    = {27},
  number    = {2},
  pages     = {443--460},
  year      = {2014},
  publisher = {IEEE}
}

@article{Khashabi2019OnTC,
  title   = {On the Capabilities and Limitations of Reasoning for Natural Language Understanding},
  author  = {Daniel Khashabi and Erfan Sadeqi Azer and Tushar Khot and Ashutosh Sabharwal and Dan Roth},
  journal = {CoRR},
  year    = {2019},
  volume  = {abs/1901.02522}
}

@article{Ando2005,
  acmid      = {1194905},
  author     = {Ando, Rie Kubota and Zhang, Tong},
  issn       = {1532-4435},
  issue_date = {12/1/2005},
  journal    = {Journal of Machine Learning Research},
  month      = dec,
  numpages   = {37},
  pages      = {1817--1853},
  publisher  = {JMLR.org},
  title      = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
  volume     = {6},
  year       = {2005}
}

@inproceedings{P16-1001,
  author    = {Goodman, James
	and Vlachos, Andreas
	and Naradowsky, Jason},
  title     = {Noise reduction and targeted exploration in imitation learning for      Abstract Meaning Representation parsing    },
  booktitle = {ACL},
  year      = {2016},
  publisher = {Association for Computational Linguistics},
  pages     = {1--11},
  location  = {Berlin, Germany},
  doi       = {10.18653/v1/P16-1001}
}

@inproceedings{C14-1001,
  author    = {Harper, Mary},
  title     = {Learning from 26 Languages: Program Management and Science in the Babel Program},
  booktitle = {COLING},
  year      = {2014},
  publisher = {Dublin City University and Association for Computational Linguistics},
  pages     = {1},
  location  = {Dublin, Ireland}
}

@article{Sakaguchi2019WINOGRANDEAA,
  title   = {WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale},
  author  = {Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1907.10641}
}

@article{susanto2020lexically,
  title   = {Lexically Constrained Neural Machine Translation with Levenshtein Transformer},
  author  = {Susanto, Raymond Hendy and Chollampatt, Shamil and Tan, Liling},
  journal = {arXiv preprint arXiv:2004.12681},
  year    = {2020}
}


@string{emnlp = "Empirical Methods in Natural Language Processing (EMNLP)"}

@inproceedings{li2020efficient,
  title     = {Efficient One-Pass End-to-End Entity Linking for Questions},
  author    = {Li, Belinda Z. and Min, Sewon and Iyer, Srinivasan and Mehdad, Yashar and Yih, Wen-tau},
  booktitle = emnlp,
  year      = {2020}
}

@article{davis2015commonsense,
  title     = {Commonsense reasoning and commonsense knowledge in artificial intelligence},
  author    = {Davis, Ernest and Marcus, Gary},
  journal   = {Communications of the ACM},
  volume    = {58},
  number    = {9},
  pages     = {92--103},
  year      = {2015},
  publisher = {ACM New York, NY, USA}
}

@article{davis2015commonsense,
  title     = {Commonsense reasoning and commonsense knowledge in artificial intelligence},
  author    = {Davis, Ernest and Marcus, Gary},
  journal   = {Communications of the ACM},
  volume    = {58},
  number    = {9},
  pages     = {92--103},
  year      = {2015},
  publisher = {ACM New York, NY, USA}
}

@misc{he2021deberta,
      title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention}, 
      author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2006.03654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{zhang2019heterogeneous,
        author = {Zhang, Chuxu and Song, Dongjin and Huang, Chao and Swami, Ananthram and Chawla, Nitesh V},
        booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
        date-added = {2021-04-03 01:39:20 +0800},
        date-modified = {2021-04-03 01:44:13 +0800},
        keywords = {Recommender system, Graph Neural Network},
        pages = {793--803},
        title = {Heterogeneous graph neural network},
        year = {2019},
        Bdsk-Url-1 = {https://doi.org/10.1145/3292500.3330961}}